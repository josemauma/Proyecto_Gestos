{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731323752.542367 2278366 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1731323752.566327 2278594 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731323752.574086 2278592 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-11-11 12:15:53.144 python[73733:2278366] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-11-11 12:15:53.144 python[73733:2278366] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "W0000 00:00:1731323763.671262 2278597 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeteccion de gesto\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# Salir al presionar la tecla 'q'\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Liberar recursos\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Configuraciones de MediaPipe para la detección de manos\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Inicializar la captura de video de la cámara\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Iniciar la detección de manos con MediaPipe\n",
    "with mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convertir la imagen de BGR a RGB para MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Si se detecta una mano en la imagen\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Dibujar las marcas en la mano\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Obtener la posición de las marcas relevantes en los dedos\n",
    "                thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y\n",
    "                index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
    "                middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y\n",
    "                ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP].y\n",
    "                pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].y\n",
    "                thumb_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP].y  # Base del pulgar\n",
    "\n",
    "                # Verificar que solo el índice y el medio están levantados (gesto de paz)\n",
    "                if (index_tip < thumb_tip and index_tip < ring_tip and\n",
    "                    middle_tip < thumb_tip and middle_tip < ring_tip and\n",
    "                    ring_tip > index_tip and pinky_tip > index_tip):\n",
    "                    cv2.rectangle(frame, (45, 20), (300, 70), (0, 0, 0), -1)  # Fondo negro para el texto\n",
    "                    cv2.putText(frame, 'Signo de la paz', (50, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Verificar el gesto de pulgar hacia arriba (thumb arriba y los otros dedos abajo)\n",
    "                if (thumb_tip < thumb_mcp and index_tip > thumb_tip and\n",
    "                    middle_tip > thumb_tip and ring_tip > thumb_tip and pinky_tip > thumb_tip):\n",
    "                    cv2.rectangle(frame, (45, 80), (200, 130), (0, 0, 0), -1)  # Fondo negro para el texto\n",
    "                    cv2.putText(frame, 'OK', (50, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                    \n",
    "                if (index_tip < hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].y and\n",
    "                    thumb_tip < thumb_mcp and\n",
    "                    middle_tip > hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].y and\n",
    "                    ring_tip > hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP].y and\n",
    "                    pinky_tip > hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP].y):\n",
    "                    cv2.rectangle(frame, (45, 260), (200, 310), (0, 0, 0), -1)  # Fondo negro para el texto\n",
    "                    cv2.putText(frame, 'Pistola', (50, 300), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Mostrar el video en tiempo real\n",
    "        cv2.imshow('Deteccion de gesto', frame)\n",
    "\n",
    "        # Salir al presionar la tecla 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
